import os
import logging
from fastapi import FastAPI, Query, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from openai import OpenAI
from dotenv import load_dotenv

# ✅ 1. Load environment variables
load_dotenv()

# ✅ 2. Configure logging for Render
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("bible_ai")

# ✅ 3. Initialize FastAPI
app = FastAPI(title="Bible AI Backend", version="2.0")

# ✅ 4. CORS Middleware for frontend access
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all for simplicity; restrict to your frontend in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ✅ 5. OpenAI Client
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise RuntimeError("OPENAI_API_KEY not found. Please set it in Render env variables.")

client = OpenAI(api_key=api_key)


# ✅ 6. Health check endpoint
@app.get("/")
def home():
    return {"message": "Bible AI backend is running!"}


# ✅ 7. Main Bible AI endpoint
@app.get("/bible")
def bible(query: str = Query(..., description="Enter your Bible question here")):
    """
    Ask Bible AI a question. Returns a clean answer in JSON.
    """
    try:
        # Log the question for debugging
        logger.info(f"Received query: {query}")

        # Ask OpenAI
        response = client.responses.create(
            model="gpt-4.1-mini",
            input=f"Answer this based on the Bible only: {query}",
            temperature=0.7,  # slightly creative but reliable
        )

        answer = response.output_text.strip()

        if not answer:
            raise HTTPException(status_code=500, detail="No answer generated by AI.")

        # Return clean JSON
        return {
            "status": "success",
            "question": query,
            "answer": answer
        }

    except Exception as e:
        logger.error(f"Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))







